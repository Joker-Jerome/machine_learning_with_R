---
title: "BST-644: Final Project"
author: "Yian Lin"
date: "3/10/2018"
output: 
  html_document:
    theme: cosmo
  pdf_document: default
---
### Problem 1
#####(a) Load the data
```{r 1.a}
load("/Users/jerome/Downloads/nhanes2003-2004.Rda")
d <- nhanes2003_2004
# Remove the observations with missing RIDAGEEX
d1 <- d[!(is.na(d$RIDAGEEX)), ]
```

We obtain the dataset and remove the observations with missing RIDAGEEX (outcome of interest).

#####(b) Missingness
```{r 1.b}
# Remove the variables with >10% missing values
miss <- rep(NA, ncol(d1))
for (i in 1:ncol(d1)) {
  miss[i] <- sum(is.na(d1[, i]))
}
missless10 <- miss < nrow(d1) * 0.1
d1 <- d1[, missless10]
# Remove the variables with less to none prediction function
cnames <- colnames(d1)
d1 <- d1[, is.na(match(cnames, c("SEQN", "SDDSRVYR", "RIDSTATR", 
                                "RIDAGEYR", "RIDRETH2", "WTINT2YR",
                                "WTMEC2YR", "SDMVPSU", "SDMVSTRA",
                                "WTDRD1", "DRDINT")))]
# Plot the missing patterns of the samples
missv <- rep(NA, nrow(d1))
for (i in 1:nrow(d1)) {
  missv[i] <- sum(is.na(d1[i, ]))
}
library(ggplot2)
qplot(missv, geom = "histogram")
library(Amelia)
library(mlbench)
missmap(d1[missv > 0, 1:50], 
        col = c("black", "grey"), legend = FALSE)
# Get a set of complete data
d11 <- d1[complete.cases(d1), ]
# nlevels
levels_vec <- sapply(d11,nlevels)
# Convert the data type
```

```{r}
# convert
for (i in 1:ncol(d11)) {
  if (levels_vec[i] > 15 & i!= 11) {
    d11[, i] <- scale(as.numeric(d11[, i]))
  }
}

d11[,11] <- as.numeric(d11[,11])
```

We remove the variables with >10% missing values as well as variables with less to none prediction function (respondent sequence number, data release number, interview/examination status, age at screening in years (since we have information in months), linked NH3 race (since we have information in race), full sample 2 year interview weight, full sample 2 year MEC exam weight, masked variance pseudo-psu, masked variance pseudo-stratum, dietary day one sample weight, and the variable indicating whether the sample person has intake data for one or two days), and plot out a histogram of number of missing variable values, as well as the patterns of missingness of the remaining first 50 variables for all observations.  From the histogram, we can see that the majority (6880) of the observations now have complete information for each variable, which are the ones we are going to use to build the prediction model.

#####(c) Ridge regression
```{r 1.c}
grid <- 10^seq(10, -2, length = 100)
# Cross validation
set.seed(2)
total.mat <- model.matrix(RIDAGEEX ~ ., data = d11)
cv.ridge <- cv.glmnet(total.mat, d11$RIDAGEEX,
                      alpha = 0, lambda = grid, thresh = 1e-12)
plot(cv.ridge)
olambda <- cv.ridge$lambda.min
olambda
# Min of the CV MSE
min(cv.ridge$cvm)
# Refit Ridge
fit.ridge <- glmnet(total.mat, d11$RIDAGEEX,
                    alpha = 0, lambda = grid, thresh = 1e-12)
predict(fit.ridge, s = olambda, type = "coefficients")
```

Using Ridge regression, with the optimal $\lambda$ being `r olambda`, the 10-fold cross-validated MSE is `r min(cv.ridge$cvm)`.

#####(c) Ridge regression

### Problem 2

```{r}
# elastic net 
set.seed(2)
a <- seq(0.05, 0.95, 0.05)
search <- foreach(i = a, .combine = rbind) %dopar% {
    cv <- cv.glmnet(total.mat, d11$RIDAGEEX,
                    alpha = i, lambda = grid, thresh = 1e-12)
    data.frame(cvm = cv$cvm[cv$lambda == cv$lambda.min], lambda.min = cv$lambda.min, alpha = i)
}
cv3 <- search[search$cvm == min(search$cvm), ]
olambda <- cv3$lambda.min
oalpha <- cv3$alpha

# mse
cv3$cvm

```

##### SVR
```{r}
library(e1071)
modelsvm <- svm(RIDAGEEX~.,data = d11)
predYsvm <- predict(modelsvm, d11)
mean((d11$RIDAGEEX-predYsvm)^2)

```
```{r}
library(gbm)
set.seed(2)
modelGBM <- gbm(RIDAGEEX~.,data = d11, distribution = "gaussian",interaction.depth = 3, shrinkage = 0.01, n.trees=1000, cv.folds=10)
predYgbm <- predict(modelGBM, d11)
mean((d11$RIDAGEEX-predYgbm)^2)

# cv mse
modelGBM$cv.error[1000]
```


### Binary Classification
```{r}
d2 <- d[!(is.na(d$mortstat)), ]
miss <- rep(NA, ncol(d2))
for (i in 1:ncol(d2)) {
  miss[i] <- sum(is.na(d2[, i]))
}
missless10 <- miss < nrow(d2) * 0.1
d2_v1 <- d2[, missless10]
# Remove the variables with less to none prediction function
cnames <- colnames(d2_v1)
d2_v2 <- d2_v1[, is.na(match(cnames, c("SEQN", "SDDSRVYR", "RIDSTATR", 
                                "RIDAGEYR", "RIDRETH2", "WTINT2YR",
                                "WTMEC2YR", "SDMVPSU", "SDMVSTRA",
                                "WTDRD1", "DRDINT")))]
# Plot the missing patterns of the samples
missv <- rep(NA, nrow(d2_v2))
for (i in 1:nrow(d2_v2)) {
  missv[i] <- sum(is.na(d2_v2[i, ]))
  if (i%%100 == 0) {
      cat("------------------")
      cat(i/nrow(d2_v2) * 100)
      cat(" percent processed")
      cat("------------------\n")
  }
}

d2_v3 <- d2_v2[complete.cases(d2_v2), ]

# nlevels
levels_vec <- sapply(d2_v3,nlevels)

d2_v4 <- d2_v3
# Convert the data type
for (i in 1:ncol(d2_v4)) {
  if (levels_vec[i] > 15) {
    d2_v4[, i] <- scale(as.numeric(d2_v4[, i]))
  }
}

d2_sub <- d2_v4[as.numeric(d2_v3$RIDAGEMN) >= 50*12, ]

# generate the fold
library(dplyr)
n_sample <- nrow(d2_sub)
irand <- sample(1:n_sample,n_sample,replace = F)
fold_idx <- ntile(irand,10)
```

```{r}
# cv 
acc_tmp <- c()
for (i in 1:10) {
    cat("working on the")
    cat(i)
    cat(" fold.\n")
    thistrain <- d2_sub[fold_idx != i,]
    thisvalid <- d2_sub[fold_idx == i,]
    modelsvm <- svm(mortstat~.,data = thistrain)
    predYsvm <- ifelse(predict(modelsvm,thisvalid) > 0.6, 1,0)
    Yvalid <- thisvalid$mortstat
    acc_tmp <- c(acc_tmp,mean(predYsvm == Yvalid))
}
   
```


### gbm
```{r}
set.seed(2)
modelGBM <- gbm(mortstat~.,data = d2_sub, interaction.depth = 3, shrinkage = 0.01, n.trees=1000, cv.folds=10)
predYgbm <- ifelse(predict(modelGBM, d2_sub, type = "response")>0.5, 1, 0)
mean((d2_sub$mortstat==predYgbm))

# cv mse
modelGBM$cv.error[1000]
```
```{r}
t3 <- tune(svm, mortstat~.,data = d2_sub, kernel="radial", ranges=list(gamma=c( 0.005, 0.01, 0.015), cost=c(1, 2, 3)), tunecontrol=tune.control(cross=10))
summary(t3)

```

```{r}
library(xgboost)


```


```{r}
cv_group = ntile(1:20,10)
```


```{R}
library(caret)
```